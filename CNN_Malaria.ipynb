{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Normalization, Dense, InputLayer,Conv2D, MaxPool2D, Flatten, BatchNormalization,Input,Layer\n",
    "from tensorflow.keras.losses import MeanSquaredError, Huber, MeanAbsoluteError, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset_info = tfds.load('malaria', with_info=True, as_supervised=True, shuffle_files=True, split=['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):\n",
    "  dataset_size = len(dataset)\n",
    "\n",
    "  train_dataset = dataset.take(int(TRAIN_RATIO * dataset_size))\n",
    "\n",
    "  val_test_dataset = dataset.skip(int(TRAIN_RATIO * dataset_size))\n",
    "  val_dataset = val_test_dataset.take(int(VAL_RATIO * dataset_size))\n",
    "\n",
    "  test_dataset = val_test_dataset.skip(int(VAL_RATIO * dataset_size))\n",
    "  return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def resize_rescale(image, label):\n",
    "  image = tf.image.resize(image, (224,224))/255.0\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "train_dataset , val_dataset, test_dataset = splits(dataset[0], TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n",
    "train_dataset = train_dataset.map(resize_rescale)\n",
    "val_dataset = val_dataset.map(resize_rescale)\n",
    "test_dataset = test_dataset.map(resize_rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(buffer_size=8,\n",
    "                                    reshuffle_each_iteration=True).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.shuffle(buffer_size=8,\n",
    "                                    reshuffle_each_iteration=True).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEQUENTIAL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model = tf.keras.Sequential([\n",
    "    InputLayer(input_shape = (224,224,3)),\n",
    "    tf.keras.layers.Conv2D(filters = 6, kernel_size=3, strides=1, padding=\"valid\",activation ='relu'),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters = 16, kernel_size=3, strides=1, padding=\"valid\",activation ='relu'),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(100, activation =\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation =\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, activation =\"sigmoid\")])\n",
    "lenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model.compile(optimizer = Adam(learning_rate=0.01),\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lenet_model.fit(train_dataset, validation_data = val_dataset, epochs = 5, verbose = 1)\n",
    "\n",
    "test_dataset= test_dataset.batch(1)\n",
    "\n",
    "lenet_model.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model.save('C:/Users/Admin/Desktop/Tensor/lenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = tf.keras.models.load_model('lenet.h5')\n",
    "abc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONAL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_input = Input(shape = (224,224,3), name='input image')\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 6, kernel_size=3, strides=1, padding=\"valid\",activation ='relu')(func_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2,strides=2)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 16, kernel_size=3, strides=1, padding=\"valid\",activation ='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "output = tf.keras.layers.MaxPool2D(pool_size=2,strides=2)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extract_model = Model(func_input, output,name='Feature_Extractor')\n",
    "feature_extract_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_input = Input(shape = (224,224,3), name='input image')\n",
    "\n",
    "x = feature_extract_model(func_input)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(100, activation =\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(10, activation =\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "func_output = Dense(1, activation =\"sigmoid\")(x)\n",
    "\n",
    "lenet_model1 = Model(func_input, func_output, name= 'Lenet_Model')\n",
    "lenet_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model1.compile(optimizer = Adam(learning_rate=0.01),\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= ['accuracy']\n",
    ")\n",
    "history = lenet_model1.fit(train_dataset, validation_data = val_dataset, epochs = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL SUBCLASSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, padding, activation, pool_size,):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv_1 = Conv2D(filters = 8, kernel_size = 3, strides = 1, padding = 'valid', activation = 'relu')\n",
    "        self.batch_1 = BatchNormalization()\n",
    "        self.pool_1 = MaxPool2D (pool_size = 2, strides= 2)\n",
    "\n",
    "        self.conv_2 = Conv2D(filters = 16, kernel_size = 3, strides = 1, padding = 'valid', activation = 'relu')\n",
    "        self.batch_2 = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D (pool_size = 2, strides= 2)\n",
    "    def call(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_1(x)\n",
    "        x = self.pool_1(x)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = self.batch_2(x)\n",
    "        x = self.pool_2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "func_input = tf.keras.Input(shape = (224,224,3), name='input image')\n",
    "\n",
    "feature_sub_classed = FeatureExtractor(8,3,1,\"valid\",\"relu\",2)\n",
    "\n",
    "x = feature_sub_classed(func_input)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(100, activation =\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(10, activation =\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "func_output = Dense(1, activation =\"sigmoid\")(x)\n",
    "\n",
    "lenet_subclass_model = Model(func_input, func_output, name= 'Lenet_Model2')\n",
    "lenet_subclass_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_subclass_model.compile(optimizer = Adam(learning_rate=0.01),\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= ['accuracy']\n",
    ")\n",
    "history = lenet_subclass_model.fit(train_dataset, validation_data = val_dataset, epochs = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LenetModel(Model):\n",
    "    def __init__(self):\n",
    "        super(LenetModel,self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(8, 3, 1, \"valid\", \"relu\", 2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.dense_1 = Dense(100, activation = \"relu\")\n",
    "        self.batch_1 = BatchNormalization()\n",
    "\n",
    "        self.dense_2 = Dense(10, activation = \"relu\")\n",
    "        self.batch_2 = BatchNormalization()\n",
    "\n",
    "        self.dense_3 = Dense(1, activation = \"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.batch_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.batch_2(x)\n",
    "        x = self.dense_3(x)\n",
    "       \n",
    "        return x\n",
    "lenet_sub_classed = LenetModel()\n",
    "lenet_sub_classed(tf.ones([1,224,224,3]))\n",
    "lenet_sub_classed.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_sub_classed.compile(optimizer = Adam(learning_rate=0.01),\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_sub_classed.fit(train_dataset, validation_data = val_dataset, epochs = 5, verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
