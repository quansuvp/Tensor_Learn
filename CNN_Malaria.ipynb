{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import albumentations as A\n",
    "import cv2 as cv\n",
    "import io\n",
    "import wandb\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Normalization, Dense, InputLayer,Conv2D, MaxPool2D, Flatten, BatchNormalization,Input,Layer, Dropout, RandomFlip, RandomRotation, Resizing, Rescaling\n",
    "from tensorflow.keras.losses import MeanSquaredError, Huber, MeanAbsoluteError, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError,binary_accuracy, BinaryAccuracy, FalseNegatives, FalsePositives, TruePositives, TrueNegatives, Precision, Recall, AUC\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from wandb.integration.keras import WandbCallback, WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandb Install, Login and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Admin\\Desktop\\Tensor\\wandb\\run-20240710_120643-ia2ap70z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb/runs/ia2ap70z' target=\"_blank\">laced-eon-9</a></strong> to <a href='https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb' target=\"_blank\">https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb/runs/ia2ap70z' target=\"_blank\">https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb/runs/ia2ap70z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CANNOT RUN =))\n",
    "wandb.login()\n",
    "wandb.require(\"core\")\n",
    "run = wandb.init(project='CNN_Malaria.ipynb',entity='khangqn1212-learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"LEARNING_RATE\": 0.01,\n",
    "  \"N_EPOCHS\": 1,\n",
    "  \"BATCH_SIZE\": 128,\n",
    "  \"DROPOUT_RATE\": 0.0,\n",
    "  \"IM_SIZE\": 224,\n",
    "  \"REGULARIZATION_RATE\": 0.0,\n",
    "  \"N_FILTERS\": 6,\n",
    "  \"KERNEL_SIZE\": 3,\n",
    "  \"N_STRIDES\": 1,\n",
    "  \"POOL_SIZE\": 2,\n",
    "  \"N_DENSE_1\": 100,\n",
    "  \"N_DENSE_2\": 10,\n",
    "}\n",
    "CONFIGURATION = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset_info = tfds.load('malaria', with_info=True, as_supervised=True, shuffle_files=True, split=['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.imgae.adjust_brightness\n",
    "# tf.imgae.adjust_contrast\n",
    "# tf.imgae.adjust_gamma\n",
    "# tf.imgae.adjust_saturation\n",
    "# tf.imgae.flip_left_right\n",
    "# tf.imgae.flip_up_down\n",
    "# tf.imgae.rot90\n",
    "\n",
    "##### INSTEAD OF AUGMENT IMAGE MANUALLY WE CAN USE KERAS LAYER AND PUT IT IN THE MODEL\n",
    "\n",
    "# tf.keras.layers.RandomContrast\n",
    "# tf.keras.layers.RandomCrop\n",
    "# tf.keras.layers.RandomFlip\n",
    "# tf.keras.layers.RandomRotation\n",
    "# tf.keras.layers.RandomTranslation\n",
    "# tf.keras.layers.RandomZoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(original, augmented):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(original)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(augmented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function : convert function from eager mode to graph mode -> better performance\n",
    "def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):\n",
    "  dataset_size = len(dataset)\n",
    "\n",
    "  train_dataset = dataset.take(int(TRAIN_RATIO * dataset_size))\n",
    "\n",
    "  val_test_dataset = dataset.skip(int(TRAIN_RATIO * dataset_size))\n",
    "  val_dataset = val_test_dataset.take(int(VAL_RATIO * dataset_size))\n",
    "\n",
    "  test_dataset = val_test_dataset.skip(int(VAL_RATIO * dataset_size))\n",
    "  return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "@tf.function\n",
    "def resize_rescale(image, label):\n",
    "  image = tf.image.resize(image, (224,224))/255.0\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "\n",
    "train_dataset , val_dataset, test_dataset = splits(dataset[0], TRAIN_RATIO, VAL_RATIO, TEST_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image, label = next(iter(train_dataset))\n",
    "# augmented_image = tf.image.flip_left_right(original_image)\n",
    "# augmented_image = tf.image.random_flip_up_down(original_image)\n",
    "# augmented_image = tf.image.rot90(original_image)\n",
    "# augmented_image = tf.image.adjust_brightness(original_image, 0.8)\n",
    "# augmented_image = tf.image.random_saturation(original_image, lower = 0, upper = 1)\n",
    "augmented_image = tf.image.central_crop(original_image, 0.8)\n",
    "\n",
    "visualize(original_image, augmented_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUGMENTATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tf.image augment\n",
    "@tf.function\n",
    "def augment(image, label):\n",
    "    image, label = resize_rescale(image, label)\n",
    "    image = tf.image.rot90(image, k= tf.random.uniform(shape=[], minval=0,maxval=2,dtype=tf.int32))\n",
    "    # k = probability\n",
    "    # image = tf.image.adjust_saturation(image, 0.3) -> may lead to poor performance\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUGMENTATION LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tf.keras.layer augment\n",
    "resize_rescale_layers = tf.keras.Sequential([\n",
    "    Resizing(224,224),\n",
    "    Rescaling(1.0/255)\n",
    "])\n",
    "\n",
    "augment_layers = tf.keras.Sequential([\n",
    "    RandomRotation(factor = (0.25, 0.2501),),\n",
    "    RandomFlip(mode = \"horizontal\",)\n",
    "\n",
    "])\n",
    "\n",
    "@tf.function\n",
    "def augment_layer(image, labels):\n",
    "    return augment_layers(resize_rescale_layers(image), training = True) , labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIXUP DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1 = train_dataset.shuffle(buffer_size=4096).map(resize_rescale)\n",
    "train_dataset_2 = train_dataset.shuffle(buffer_size=4096).map(resize_rescale)\n",
    "\n",
    "mixed_dataset = tf.data.Dataset.zip((train_dataset_1,train_dataset_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def mixup(train_dataset_1, train_dataset_2):\n",
    "    (image_1, label_1) ,(image_2,label_2) = train_dataset_1, train_dataset_2\n",
    "    \n",
    "    lamda = tfp.distributions.Beta(0.2, 0.2)\n",
    "    lamda = lamda.sample(1)[0]\n",
    "\n",
    "    label_1 = tf.cast(label_1,dtype= tf.dtypes.float32)\n",
    "    label_2 = tf.cast(label_2,dtype= tf.dtypes.float32)\n",
    "    image = lamda*image_1 + (1-lamda)*image_2\n",
    "    label = lamda*label_1 + (1-lamda)*label_2\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUTMIX AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZE = 224\n",
    "@tf.function\n",
    "def box(lamda):\n",
    "    r_x = tf.cast(tfp.distributions.Uniform(0, IM_SIZE ).sample(1)[0], dtype= tf.int32)\n",
    "    r_y = tf.cast(tfp.distributions.Uniform(0, IM_SIZE ).sample(1)[0], dtype= tf.int32)\n",
    "\n",
    "    r_w = tf.cast(IM_SIZE*tf.math.sqrt(1-lamda), dtype = tf.int32)\n",
    "    r_h = tf.cast(IM_SIZE*tf.math.sqrt(1-lamda), dtype = tf.int32)\n",
    "\n",
    "    r_x = tf.clip_by_value(r_x - r_w//2, 0, IM_SIZE)\n",
    "    r_y = tf.clip_by_value(r_y - r_h//2, 0, IM_SIZE)\n",
    "\n",
    "    x_b_r = tf.clip_by_value(r_x + r_w//2, 0, IM_SIZE)\n",
    "    y_b_r = tf.clip_by_value(r_y + r_h//2, 0, IM_SIZE)\n",
    "\n",
    "    r_w = x_b_r - r_x\n",
    "    if(r_w == 0):\n",
    "        r_w = 1\n",
    "    r_h = y_b_r - r_y\n",
    "    if(r_h ==0):\n",
    "        r_h = 1\n",
    "    return r_y, r_x, r_h, r_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def cutmixup(train_dataset_1, train_dataset_2):\n",
    "    (image_1, label_1) ,(image_2,label_2) = train_dataset_1, train_dataset_2\n",
    "    \n",
    "    lamda = tfp.distributions.Beta(0.2,0.2)\n",
    "    lamda = lamda.sample(1)[0]\n",
    "\n",
    "    r_y, r_x , r_h, r_w = box(lamda)\n",
    "    cut_cat = tf.image.crop_to_bounding_box(image_1, r_y, r_x, r_h, r_w)\n",
    "    cut_dog = tf.image.crop_to_bounding_box(image_2,  r_y, r_x, r_h, r_w)\n",
    "    pad_dog = tf.image.pad_to_bounding_box(cut_dog,  r_y, r_x, IM_SIZE,IM_SIZE)\n",
    "    pad_cat = tf.image.pad_to_bounding_box(cut_cat,  r_y, r_x, IM_SIZE,IM_SIZE)\n",
    "    image = image_2 - pad_dog + pad_cat\n",
    "    lamda = tf.cast(1- (r_w*r_h)/(IM_SIZE*IM_SIZE), dtype = tf.float32)\n",
    "    label = lamda*tf.cast(label_1, dtype = tf.float32) + (1-lamda)*tf.cast(label_2, dtype = tf.float32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### WORK FOR OTHER PROBLEM NOT THIS ONE SO IT LEAD TO POOR PERFORMANCE\n",
    "# train_dataset = mixed_dataset.shuffle(\n",
    "#     buffer_size=8,\n",
    "#     reshuffle_each_iteration=True).map(mixup).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "# train_dataset = mixed_dataset.shuffle(\n",
    "#     buffer_size=8,\n",
    "#     reshuffle_each_iteration=True).map(cutmixup).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original ,label = next(iter(train_dataset))\n",
    "print(label)\n",
    "plt.imshow(original[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUGMENTATION WITH ALBUMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose(\n",
    "    [\n",
    "      A.Resize(IM_SIZE, IM_SIZE),\n",
    "\n",
    "      A.OneOf([A.HorizontalFlip(),\n",
    "                A.VerticalFlip(),], p = 0.3),\n",
    "      \n",
    "      A.RandomRotate90(),   \n",
    "      A.RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                contrast_limit=0.2,\n",
    "                                always_apply=False, p=0.5),\n",
    "      A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), always_apply=False, p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def aug_albument(image):\n",
    "    data = {'image':image}\n",
    "    image = transforms(**data)\n",
    "    image = image[\"image\"]\n",
    "    image = tf.cast(image/255, tf.float32)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def process_data(image, label):\n",
    "    aug_img = tf.numpy.function(func = aug_albument, inp=[image], Tout= tf.float32)\n",
    "    return aug_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(\n",
    "    buffer_size=8,\n",
    "    reshuffle_each_iteration=True).map(process_data).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE TRAIN AND VAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(\n",
    "    buffer_size=8,\n",
    "    reshuffle_each_iteration=True).map(augment_layer).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.shuffle(\n",
    "    buffer_size=8,\n",
    "    reshuffle_each_iteration=True).map(resize_rescale).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.map(resize_rescale)\n",
    "\n",
    "# instead of doing augmentation using map as above\n",
    "# we can embedd the aumgnet_layer into the model itself\n",
    "# then we can remove the map(augmnet_layer, resize_rescale) above\n",
    "# remember to do this for test_dataset also (cause the layer already in the model)\n",
    "# and one important thing if we use keras.layer augment we use batch_size = 1 -> .batch(1) not 32\n",
    "# and an extra thing we can add probability that if the image is augmented at some random epochs or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEQUENTIAL API (+ DROPOUT LAYER & REGULARIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout\n",
    "# add regularizer (in conv2D layer, dense layer) -> kernel_regularizer\n",
    "# add augmentation layer\n",
    "IM_SIZE = CONFIGURATION['IM_SIZE']\n",
    "DROPOUT_RATE = CONFIGURATION['DROPOUT_RATE']\n",
    "REGULARIZATION_RATE = CONFIGURATION['REGULARIZATION_RATE']\n",
    "N_FILTERS = CONFIGURATION['N_FILTERS']\n",
    "KERNEL_SIZE = CONFIGURATION['KERNEL_SIZE']\n",
    "POOL_SIZE = CONFIGURATION['POOL_SIZE']\n",
    "N_STRIDES = CONFIGURATION['N_STRIDES']\n",
    "N_DENSE_1 = CONFIGURATION['N_DENSE_1']\n",
    "N_DENSE_2 = CONFIGURATION['N_DENSE_2']\n",
    "\n",
    "lenet_model = Sequential([\n",
    "    InputLayer(input_shape = (224,224,3)),\n",
    "    \n",
    "    # resize_rescale_layers,\n",
    "    # augment_layers,\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters = 6, kernel_size=3, strides=1, padding=\"valid\",activation ='relu',kernel_regularizer=L2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n",
    "    Dropout(rate = 0.3),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters = 16, kernel_size=3, strides=1, padding=\"valid\",activation ='relu',kernel_regularizer=L2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(100, activation =\"relu\",kernel_regularizer=L2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate = 0.3),\n",
    "    Dense(10, activation =\"relu\",kernel_regularizer=L2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, activation =\"sigmoid\")])\n",
    "lenet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM LOSS METHOD (WITHOUT PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def custom_bce(y_true, y_pred):\n",
    "  bce = BinaryCrossentropy()\n",
    "  return bce(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM LOSS METHOD (WITH PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 1\n",
    "@tf.function\n",
    "def custom_bce(FACTOR):\n",
    "    def loss(y_true, y_pred):\n",
    "        bce = BinaryCrossentropy()\n",
    "        return bce(y_true, y_pred)*FACTOR\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM LOSS CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 1\n",
    "class CustomBCE(tf.keras.losses.Loss):\n",
    "    def __init__(self, FACTOR):\n",
    "        super(CustomBCE,self).__init__()\n",
    "        self.FACTOR = FACTOR\n",
    "    def call(self, y_true, y_pred):\n",
    "        bce = BinaryCrossentropy()\n",
    "        return bce(y_true, y_pred)*self.FACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM METRIC METHOD (WITHOUT PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    return binary_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM METRIC METHOD (with PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 1\n",
    "@tf.function\n",
    "def custom_accuracy(FACTOR):\n",
    "    def metric(y_true, y_pred):\n",
    "        return binary_accuracy(y_true, y_pred)* FACTOR\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM METRIC CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 1\n",
    "class CustomAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name = \"Custom_Accuracy\" ,FACTOR = 1):\n",
    "        super(CustomAccuracy,self).__init__()\n",
    "        self.FACTOR = FACTOR\n",
    "        self.accuracy = self.add_weight(name= name, initializer= 'zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        self.accuracy = binary_accuracy(tf.cast(y_true,dtype = tf.float32), y_pred)* self.FACTOR\n",
    "    def result(self):\n",
    "        return self.accuracy\n",
    "    def reset(self):\n",
    "        self.accuracy.assign(0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model.compile(optimizer = Adam(learning_rate=0.01),\n",
    "            loss = CustomBCE(FACTOR),\n",
    "            metrics= [CustomAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lenet_model.fit(train_dataset, validation_data = val_dataset, epochs = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMTRAIN_DIR = './logs/custom/train'\n",
    "CUSTOMVAL_DIR = './logs/custom/val'\n",
    "\n",
    "custom_train_writer = tf.summary.create_file_writer(CUSTOMTRAIN_DIR)\n",
    "custom_val_writer = tf.summary.create_file_writer(CUSTOMVAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = Adam(learning_rate=0.01)\n",
    "EPOCHS = range(3)\n",
    "METRIC = BinaryAccuracy()\n",
    "METRIC_VAL = BinaryAccuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_block(x_batch, y_batch):\n",
    "    with tf.GradientTape() as recorder:\n",
    "        y_pred = lenet_model(x_batch, training = True)\n",
    "        loss = custom_bce(y_batch, y_pred)\n",
    "        \n",
    "    partial_derivatives = recorder.gradient(loss, lenet_model.trainable_weights)\n",
    "    OPTIMIZER.apply_gradients(zip(partial_derivatives, lenet_model.trainable_weights))\n",
    "    METRIC.update_state(y_batch, y_pred)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def val_block(x_batch_val, y_batch_val):\n",
    "    y_pred_val = lenet_model(x_batch_val, training = False)\n",
    "    loss_val = custom_bce(y_batch_val, y_pred_val)\n",
    "\n",
    "    METRIC_VAL.update_state(y_batch_val, y_pred_val)\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralearn(model,loss_fucntion,METRIC, METRIC_VAL,OPTIMIZER,train_dataset, val_dataset, EPOCHS):\n",
    "    for epoch in EPOCHS:\n",
    "        print(\"Training starts for epoch number {}\".format(epoch))\n",
    "        for step, (x_batch , y_batch) in enumerate(train_dataset):\n",
    "            loss = training_block(x_batch, y_batch)\n",
    "\n",
    "        print(\"Training loss\", loss)\n",
    "        print(\"The accuracy is\", METRIC.result())\n",
    "        with custom_train_writer.as_default():\n",
    "            tf.summary.scalar('Training Loss',data = loss, step = epoch)\n",
    "        with custom_train_writer.as_default():\n",
    "            tf.summary.scalar('Training Accuracy',data = METRIC.result(), step = epoch)\n",
    "\n",
    "        METRIC.reset_state()\n",
    "        for (x_batch_val, y_batch_val) in val_dataset:\n",
    "            loss_val = val_block(x_batch_val, y_batch_val)\n",
    "        print(\"Validation loss {}\".format(loss_val))\n",
    "        print(\"The accuracy is\", METRIC_VAL.result())\n",
    "\n",
    "        with custom_val_writer.as_default():\n",
    "            tf.summary.scalar('Val Loss',data = loss_val, step = epoch)\n",
    "        with custom_val_writer.as_default():\n",
    "            tf.summary.scalar('Val Accuracy',data = METRIC_VAL.result(), step = epoch)\n",
    "        METRIC_VAL.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralearn(lenet_model,custom_bce, METRIC, METRIC_VAL,OPTIMIZER, train_dataset, val_dataset,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset =test_dataset.batch(1)\n",
    "lenet_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model.save('C:/Users/Admin/Desktop/Tensor/lenet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONAL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_input = Input(shape = (224,224,3), name='input image')\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 6, kernel_size=3, strides=1, padding=\"valid\",activation ='relu')(func_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPool2D(pool_size=2,strides=2)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters = 16, kernel_size=3, strides=1, padding=\"valid\",activation ='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "output = tf.keras.layers.MaxPool2D(pool_size=2,strides=2)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extract_model = Model(func_input, output,name='Feature_Extractor')\n",
    "feature_extract_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_input = Input(shape = (224,224,3), name='input image')\n",
    "\n",
    "x = feature_extract_model(func_input)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(100, activation =\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(10, activation =\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "func_output = Dense(1, activation =\"sigmoid\")(x)\n",
    "\n",
    "lenet_model1 = Model(func_input, func_output, name= 'Lenet_Model')\n",
    "lenet_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model1.compile(optimizer = Adam(learning_rate=0.01)\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= ['accuracy']\n",
    ")\n",
    "history = lenet_model1.fit(train_dataset, validation_data = val_dataset, epochs = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL SUBCLASSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, padding, activation, pool_size,):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv_1 = Conv2D(filters = 8, kernel_size = 3, strides = 1, padding = 'valid', activation = 'relu')\n",
    "        self.batch_1 = BatchNormalization()\n",
    "        self.pool_1 = MaxPool2D (pool_size = 2, strides= 2)\n",
    "\n",
    "        self.conv_2 = Conv2D(filters = 16, kernel_size = 3, strides = 1, padding = 'valid', activation = 'relu')\n",
    "        self.batch_2 = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D (pool_size = 2, strides= 2)\n",
    "    def call(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_1(x)\n",
    "        x = self.pool_1(x)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = self.batch_2(x)\n",
    "        x = self.pool_2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "func_input = tf.keras.Input(shape = (224,224,3), name='input image')\n",
    "\n",
    "feature_sub_classed = FeatureExtractor(8,3,1,\"valid\",\"relu\",2)\n",
    "\n",
    "x = feature_sub_classed(func_input)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(100, activation =\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(10, activation =\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "func_output = Dense(1, activation =\"sigmoid\")(x)\n",
    "\n",
    "lenet_subclass_model = Model(func_input, func_output, name= 'Lenet_Model2')\n",
    "lenet_subclass_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_subclass_model.compile(optimizer = Adam(learning_rate=0.01),\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= ['accuracy']\n",
    ")\n",
    "history = lenet_subclass_model.fit(train_dataset, validation_data = val_dataset, epochs = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LenetModel(Model):\n",
    "    def __init__(self):\n",
    "        super(LenetModel,self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(8, 3, 1, \"valid\", \"relu\", 2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.dense_1 = Dense(100, activation = \"relu\")\n",
    "        self.batch_1 = BatchNormalization()\n",
    "\n",
    "        self.dense_2 = Dense(10, activation = \"relu\")\n",
    "        self.batch_2 = BatchNormalization()\n",
    "\n",
    "        self.dense_3 = Dense(1, activation = \"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.batch_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.batch_2(x)\n",
    "        x = self.dense_3(x)\n",
    "       \n",
    "        return x\n",
    "lenet_sub_classed = LenetModel()\n",
    "lenet_sub_classed(tf.random.normal(\n",
    "    [1,224,224,3],\n",
    "    mean=0.0,\n",
    "    stddev=0.5,\n",
    "    dtype=tf.dtypes.float32,\n",
    "))\n",
    "# lenet_sub_classed.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_sub_classed.compile(optimizer = Adam(learning_rate=0.01),\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_sub_classed.fit(train_dataset, validation_data = val_dataset, epochs = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM LAYER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralearnDense(Layer):\n",
    "    def __init__(self, output_units, activation):\n",
    "        super(NeuralearnDense, self).__init__()\n",
    "        self.outout_units = output_units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_features_shape):\n",
    "        self.w = self.add_weight(shape = (input_features_shape[-1], self.outout_units), initializer = \"random_normal\", trainable = True) \n",
    "        self.b = self.add_weight(shape= (self.outout_units,), initializer = \"random_normal\", trainable = True)\n",
    "    \n",
    "    def call(self, input_features):\n",
    "\n",
    "        pre_output = tf.matmul(input_features,self.w) + self.b\n",
    "        if(self.activation == \"relu\"):\n",
    "            return tf.nn.relu(pre_output)\n",
    "        elif (self.activation == \"sigmoid\"):\n",
    "            return tf.math.sigmoid(pre_output)\n",
    "        else:\n",
    "            return pre_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_custom_model = tf.keras.Sequential([\n",
    "    InputLayer(input_shape = (224,224,3)),\n",
    "    tf.keras.layers.Conv2D(filters = 6, kernel_size=3, strides=1, padding=\"valid\",activation ='relu'),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters = 16, kernel_size=3, strides=1, padding=\"valid\",activation ='relu'),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    NeuralearnDense(100, activation =\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    NeuralearnDense(10, activation =\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    NeuralearnDense(1, activation =\"sigmoid\")])\n",
    "lenet_custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_custom_model.compile(optimizer = Adam(learning_rate=0.01),\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_custom_model.fit(train_dataset, validation_data = val_dataset, epochs = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [TruePositives(name='tp'), FalsePositives(name='fp'), TrueNegatives(name='tn') ,FalseNegatives(name='fn'),BinaryAccuracy(name='accuracy')\n",
    "           ,Precision(name='precision'), Recall(name = 'recall'), AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = tf.keras.models.load_model('lenet.h5')\n",
    "abc.compile(optimizer = Adam(learning_rate=0.01),\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZING CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "inp = []\n",
    "\n",
    "for x,y in test_dataset.as_numpy_iterator():\n",
    "    inp.append(x)\n",
    "    labels.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(inp).shape)\n",
    "print(np.array(inp)[:,0,...].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([i[0] for i in labels])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = abc.predict(np.array(inp)[:,0,...])\n",
    "print(predicted[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.999998945\n",
    "cm = confusion_matrix(labels, predicted > threshold)\n",
    "print(cm)\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "sns.heatmap(cm, annot= True)\n",
    "plt.title('Confusion_Matrix - {}'.format(threshold))\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp, tp, thresholds = roc_curve(labels, predicted)\n",
    "# print(len(fp), len(tp), len(thresholds))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(fp, tp)\n",
    "plt.xlabel('False_positive')\n",
    "plt.ylabel('True_positives')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "skip = 20\n",
    "for i in range(0, len(thresholds), skip):\n",
    "    plt.text(fp[i], tp[i], thresholds[i])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALL BACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(Callback):\n",
    "    def on_epoch_end(self,epoch, logs):\n",
    "        print(\" \\n Epoch Number {} the model has a loss {}\".format(epoch, logs['loss']))\n",
    "    def on_batch_end(self,batch, logs):\n",
    "        print(\" \\n Batch Number {} the model has a loss {}\".format(batch+1, logs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_callback = CSVLogger(\n",
    "    'logs.csv', separator=',', append = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EARLY STOPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEARNINGRATESCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch , lr):\n",
    "    if epoch < 10:\n",
    "        learning_rate = lr\n",
    "    else:\n",
    "        learning_rate = lr * tf.math.exp(-0.1)\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('Learning Rate',data = learning_rate, step = epoch)\n",
    "    return learning_rate\n",
    "lr_scheduler_callback = LearningRateScheduler(\n",
    "    schedule = scheduler,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelCheckpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'checkpoints.keras', monitor = 'val_loss', verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = False,\n",
    "    mode = \"auto\",\n",
    "    save_freq = 'epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducelr_callback= tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor = 0.1, patience=2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TENSOR BOARD INTEGRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_DIR = './logs/metrics'\n",
    "train_writer = tf.summary.create_file_writer(METRIC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = './logs', histogram_freq= 1, profile_batch='100,132')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogImageCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        labels = []\n",
    "        inp = []\n",
    "\n",
    "        for x,y in test_dataset.as_numpy_iterator():\n",
    "            inp.append(x)\n",
    "            labels.append(y)\n",
    "        labels = np.array([i[0] for i in labels])\n",
    "        predicted = abc.predict(np.array(inp)[:,0,...])\n",
    "        threshold = 0.999998945\n",
    "        cm = confusion_matrix(labels, predicted > threshold)\n",
    "        print(cm)\n",
    "        plt.figure(figsize=(4,4))\n",
    "\n",
    "        sns.heatmap(cm, annot= True)\n",
    "        plt.title('Confusion_Matrix - {}'.format(threshold))\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format = 'png')\n",
    "\n",
    "        image = tf.image.decode_png(buffer.getvalue(), channels=3)\n",
    "\n",
    "        IMAGE_DIR = './logs/images'\n",
    "        image_writer = tf.summary.create_file_writer(IMAGE_DIR)\n",
    "        image = tf.expand_dims(image, axis = 0)\n",
    "\n",
    "        with image_writer.as_default():\n",
    "            tf.summary.image('Heat Map',image, epoch)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "property 'model' of 'WandbCallback' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m abc \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlenet.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m abc\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mCONFIGURATION[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      4\u001b[0m             loss \u001b[38;5;241m=\u001b[39m BinaryCrossentropy(),\n\u001b[0;32m      5\u001b[0m             metrics\u001b[38;5;241m=\u001b[39m metrics\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m \u001b[43mabc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mWandbCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\integration\\keras\\keras.py:555\u001b[0m, in \u001b[0;36mWandbCallback.set_model\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[1;32m--> 555\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39minputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_type \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mguess_data_type(\n\u001b[0;32m    558\u001b[0m             model\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, risky\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    559\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: property 'model' of 'WandbCallback' object has no setter"
     ]
    }
   ],
   "source": [
    "run.log(CONFIGURATION)\n",
    "abc = tf.keras.models.load_model('lenet.h5')\n",
    "abc.compile(optimizer = Adam(learning_rate=CONFIGURATION['LEARNING_RATE']),\n",
    "            loss = BinaryCrossentropy(),\n",
    "            metrics= metrics\n",
    ")\n",
    "abc.fit(train_dataset, validation_data = val_dataset, epochs = 1, verbose = 1, callbacks=[WandbMetricsLogger()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BATCH_SIZE</td><td>‚ñÅ</td></tr><tr><td>DROPOUT_RATE</td><td>‚ñÅ</td></tr><tr><td>IM_SIZE</td><td>‚ñÅ</td></tr><tr><td>KERNEL_SIZE</td><td>‚ñÅ</td></tr><tr><td>LEARNING_RATE</td><td>‚ñÅ</td></tr><tr><td>N_DENSE_1</td><td>‚ñÅ</td></tr><tr><td>N_DENSE_2</td><td>‚ñÅ</td></tr><tr><td>N_EPOCHS</td><td>‚ñÅ</td></tr><tr><td>N_FILTERS</td><td>‚ñÅ</td></tr><tr><td>N_STRIDES</td><td>‚ñÅ</td></tr><tr><td>POOL_SIZE</td><td>‚ñÅ</td></tr><tr><td>REGULARIZATION_RATE</td><td>‚ñÅ</td></tr><tr><td>epoch/accuracy</td><td>‚ñÅ</td></tr><tr><td>epoch/auc</td><td>‚ñÅ</td></tr><tr><td>epoch/epoch</td><td>‚ñÅ</td></tr><tr><td>epoch/fn</td><td>‚ñÅ</td></tr><tr><td>epoch/fp</td><td>‚ñÅ</td></tr><tr><td>epoch/loss</td><td>‚ñÅ</td></tr><tr><td>epoch/precision</td><td>‚ñÅ</td></tr><tr><td>epoch/recall</td><td>‚ñÅ</td></tr><tr><td>epoch/tn</td><td>‚ñÅ</td></tr><tr><td>epoch/tp</td><td>‚ñÅ</td></tr><tr><td>epoch/val_accuracy</td><td>‚ñÅ</td></tr><tr><td>epoch/val_auc</td><td>‚ñÅ</td></tr><tr><td>epoch/val_fn</td><td>‚ñÅ</td></tr><tr><td>epoch/val_fp</td><td>‚ñÅ</td></tr><tr><td>epoch/val_loss</td><td>‚ñÅ</td></tr><tr><td>epoch/val_precision</td><td>‚ñÅ</td></tr><tr><td>epoch/val_recall</td><td>‚ñÅ</td></tr><tr><td>epoch/val_tn</td><td>‚ñÅ</td></tr><tr><td>epoch/val_tp</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BATCH_SIZE</td><td>128</td></tr><tr><td>DROPOUT_RATE</td><td>0.0</td></tr><tr><td>IM_SIZE</td><td>224</td></tr><tr><td>KERNEL_SIZE</td><td>3</td></tr><tr><td>LEARNING_RATE</td><td>0.01</td></tr><tr><td>N_DENSE_1</td><td>100</td></tr><tr><td>N_DENSE_2</td><td>10</td></tr><tr><td>N_EPOCHS</td><td>1</td></tr><tr><td>N_FILTERS</td><td>6</td></tr><tr><td>N_STRIDES</td><td>1</td></tr><tr><td>POOL_SIZE</td><td>2</td></tr><tr><td>REGULARIZATION_RATE</td><td>0.0</td></tr><tr><td>epoch/accuracy</td><td>0.94303</td></tr><tr><td>epoch/auc</td><td>0.97624</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/fn</td><td>529.0</td></tr><tr><td>epoch/fp</td><td>727.0</td></tr><tr><td>epoch/loss</td><td>0.17752</td></tr><tr><td>epoch/precision</td><td>0.93522</td></tr><tr><td>epoch/recall</td><td>0.95201</td></tr><tr><td>epoch/tn</td><td>10295.0</td></tr><tr><td>epoch/tp</td><td>10495.0</td></tr><tr><td>epoch/val_accuracy</td><td>0.94737</td></tr><tr><td>epoch/val_auc</td><td>0.98143</td></tr><tr><td>epoch/val_fn</td><td>32.0</td></tr><tr><td>epoch/val_fp</td><td>113.0</td></tr><tr><td>epoch/val_loss</td><td>0.18599</td></tr><tr><td>epoch/val_precision</td><td>0.92334</td></tr><tr><td>epoch/val_recall</td><td>0.97703</td></tr><tr><td>epoch/val_tn</td><td>1249.0</td></tr><tr><td>epoch/val_tp</td><td>1361.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-paper-8</strong> at: <a href='https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb/runs/ykldhe5a' target=\"_blank\">https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb/runs/ykldhe5a</a><br/> View project at: <a href='https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb' target=\"_blank\">https://wandb.ai/khangqn1212-learn/CNN_Malaria.ipynb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240710_115550-ykldhe5a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZATON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING Ctrl+Shift+P -> Tensorboard if using vscode\n",
    "# otherwise ruse command  tensorboard --logdir = './logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout\n",
    "# add regularizer (in conv2D layer, dense layer) -> kernel_regularizer\n",
    "# add augmentation layer\n",
    "# using hp api of tensorboard\n",
    "def model_tune(hparams):\n",
    "    lenet_model = tf.keras.Sequential([\n",
    "        InputLayer(input_shape = (224,224,3)),\n",
    "        \n",
    "        # resize_rescale_layers,\n",
    "        # augment_layers,\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters = 6, kernel_size=3, strides=1, padding=\"valid\",activation ='relu',kernel_regularizer=L2(hparams[HP_REGULARIZATION_RATE])),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n",
    "        Dropout(rate = hparams[HP_DROPOUT]),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters = 16, kernel_size=3, strides=1, padding=\"valid\",activation ='relu',kernel_regularizer=L2(hparams[HP_REGULARIZATION_RATE])),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(hparams[NUM_UNIT_1], activation =\"relu\",kernel_regularizer=L2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate = 0.3),\n",
    "        Dense(hparams[NUM_UNIT_2], activation =\"relu\",kernel_regularizer=L2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dense(1, activation =\"sigmoid\")])\n",
    "\n",
    "    lenet_model.compile(\n",
    "        optimizer = Adam(learning_rate=hparams[HP_LEARNING_RATE]),\n",
    "        loss = BinaryCrossentropy(),\n",
    "        metrics= ['accuracy']\n",
    "    )\n",
    "    lenet_model.fit(train_dataset, validation_data = val_dataset, epochs = 1, verbose = 1, callbacks=[tensorboard_callback, lr_scheduler_callback,LogImageCallback()])\n",
    "    _, accuracy = lenet_model.evaluate(val_dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_UNIT_1 = hp.HParam('num_unit_1', hp.Discrete([100]))\n",
    "NUM_UNIT_2 = hp.HParam('num_unit_2', hp.Discrete([10]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.3]))\n",
    "HP_REGULARIZATION_RATE = hp.HParam('regularization_rate', hp.Discrete([0.01]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.01]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = 0\n",
    "for num_unit_1 in NUM_UNIT_1.domain.values:\n",
    "    for num_unit_2 in NUM_UNIT_2.domain.values:\n",
    "        for dropout_rate in HP_DROPOUT.domain.values:\n",
    "            for regularization_rate in HP_REGULARIZATION_RATE.domain.values:\n",
    "                for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                    hparams = {\n",
    "                        NUM_UNIT_1 : num_unit_1,\n",
    "                        NUM_UNIT_2 : num_unit_2,\n",
    "                        HP_DROPOUT : dropout_rate,\n",
    "                        HP_REGULARIZATION_RATE: regularization_rate,\n",
    "                        HP_LEARNING_RATE: learning_rate\n",
    "                    }\n",
    "                    file_writer = tf.summary.create_file_writer('./logs' +str(run_number))\n",
    "                    with file_writer.as_default():\n",
    "                        hp.hparams(hparams)\n",
    "                        accuracy = model_tune(hparams)\n",
    "                        tf.summary.scalar('accuracy',accuracy,run_number)\n",
    "                    print('OUR HPARAM of run_number ', run_number ,hparams)\n",
    "                    run_number += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
